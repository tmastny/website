---
title: Bayesian Meta-Analysis of Periodization in Strength Training
author: ~
date: '2018-01-13'
slug: bayesian-meta-analysis-periodization-brms
categories: []
tags: []
description: Bayesian Meta-Analysis of Periodization with brms 
meta_img: /images/image.jpg
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, 
                      results='show', cache=TRUE, autodep=TRUE)
```


## Introduction

Greg Nuckols, a powerlifting coach, graduate student, and all around smart guy
just did an excellent meta-analytic study on periodization methods in strength training on his website [Strong By Science](https://www.strongerbyscience.com/periodization-data/). Instead of the classical random effects meta-analysis, Greg's methodology included looking at various permutations of percent changes in strength levels across exercises, time periods, and periodization methods. 

Consistent with open science principles, Greg shared his data set and encouraged anyone who was interested to do their own analysis. I have full confidence in Greg's conclusions, but being interested in both Bayesian statistics and powerlifting, I thought this was an excellent opportunity to combine the two pursuits.

In the article, Greg noted that he did not follow the classic methodology of a random effects model because some studies reported such small standard deviations that the effects sizes were totally implausible. 

Greg's general concern of implausible effects sizes is well warranted. The standard criteria of statistical significant almost assures that the [effect size is exaggerated](http://andrewgelman.com/2016/11/13/more-on-my-paper-with-john-carlin-on-type-m-and-type-s-errors/). Gelman and Carlin call this phenomenon a Type M (magnitude) error. I strongly recommend reading [their excellent paper](http://www.stat.columbia.edu/~gelman/research/published/retropower_final.pdf) on the subject, where they analyze suspect effect sizes using prior information.

Therefore, I believe this will be an excellent application of Bayesian Data Analysis. First, a Bayesian analysis would provide a formal way to include Greg's prior information of plausible effects sizes into the systematic analysis. Second, even without including prior information, the Bayesian approach is a natural way to apply a random effects, or multilevel, model to a meta-analysis. [Matti Vuorre](https://mvuorre.github.io/post/2016/2016-09-29-bayesian-meta-analysis/) already has an excellent blog post on this topic. 

## Data Cleaning

Before the analysis, we need to [tidy the data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). Looking at [Greg's spreadsheet](https://docs.google.com/spreadsheets/d/1uT2ZZ_PZEf_4YefPMSSxanwLWayzhwsgC0rnv6XSgU4/edit#gid=0) in Google Sheets, you can see it is optimized for human reading instead of for processing in statistical software. For example, each observation is not completely filled out: easy on the eyes, but can lead to difficulties programmatically. 

![spreadsheet picture](blog/spreadsheet_pic.png)

Overall, the spreadsheet is pretty good. It doesn't violate any of the [cardinal spreadsheet sins](http://blog.revolutionanalytics.com/2017/11/good-practices-spreadsheets.html) such as
- merged cells
- data as formatting
- formulas within raw data




