---
title: MRP Using brms and tidybayes
author: ~
date: '2017-11-20'
slug: multilevel-mrp-tidybayes-brms-stan
categories: []
tags: []
description: Multilevel Regression and Poststratification with Stan
meta_img: /images/image.jpg
draft: true
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the-data">The Data</a><ul>
<li><a href="#tidying-data">Tidying data</a></li>
</ul></li>
<li><a href="#model-1-disaggragation">Model 1: Disaggragation</a></li>
<li><a href="#model-2-maximum-likelihood-multilevel-model">Model 2: Maximum Likelihood Multilevel Model</a></li>
<li><a href="#model-3-full-bayesian-model">Model 3: Full Bayesian Model</a></li>
<li><a href="#model-comparisons">Model Comparisons</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In the last post I wrote the <a href="https://timmastny.netlify.com/blog/poststratification-with-dplyr/">“MRP Primer” Primer</a> studying the <em>p</em> part of MRP: poststratification. This post explores the actual <a href="http://www.princeton.edu/~jkastell/mrp_primer.html">MRP Primer by Jonathan Kastellec</a>. Jonathan and his coauthors wrote this excellent tutorial on Multilevel Regression and Poststratification (MRP) using <code>r-base</code> and <code>arm</code>/<code>lme4</code>.</p>
<p>Inspired by Austin Rochford’s full Bayesian implementation of the MRP Primer using <a href="https://gist.github.com/AustinRochford/bfc20cb3262169b41b730bd9faf74477">PyMC3</a>, I decided to approach the problem using R and <a href="http://mc-stan.org/">Stan</a>. In particular, I wanted to highlight two packages:</p>
<ul>
<li><p><a href="https://github.com/paul-buerkner/brms"><code>brms</code></a>, which provides a <code>lme4</code> like interface to Stan. And</p></li>
<li><p><a href="https://github.com/mjskay/tidybayes"><code>tidybayes</code></a>, which is a general tool for tidying Bayesian package outputs.</p></li>
</ul>
<p>Additionally, I’d like to compare the empirical mean disaggragated model, the maximum likelihood estimated multilevel model, the full Bayesian model.</p>
<p>Lastly, I’ll be doing some graphical map comparisons with the <code>albersusa</code> package.</p>
<p>Here’s what we’ll need to get started</p>
<pre class="r"><code>library(tidyverse)
library(lme4)
library(brms)
library(rstan)
library(albersusa)

rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores())</code></pre>
</div>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<p>Here are the three data sets we’ll need. First the <code>marriage.data</code> is a compliation of gay marriage polls that we hope can give us a perspective on the support of gay marriage state by state. <code>Statelevel</code> provides some additional state information we’ll use as predictors in our model, such as the proportion of religious voters. And just like <a href="https://timmastny.netlify.com/blog/poststratification-with-dplyr/">last time</a>, the Census data will provide our poststratification weights.</p>
<pre class="r"><code>marriage.data &lt;- foreign::read.dta(&#39;gay_marriage_megapoll.dta&#39;,
                                   convert.underscore=TRUE)
Statelevel &lt;- foreign::read.dta(&quot;state_level_update.dta&quot;,
                                convert.underscore = TRUE)
Census &lt;- foreign::read.dta(&quot;poststratification 2000.dta&quot;,
                            convert.underscore = TRUE)</code></pre>
<div id="tidying-data" class="section level3">
<h3>Tidying data</h3>
<p>Here, I am proceeding just as Jonathan Kastellec. We want to code our demographic groups in a consistent way across our data so we can easily poststratify.</p>
<pre class="r"><code>Statelevel &lt;- Statelevel %&gt;% arrange(sstate.initnum)
Census &lt;- Census %&gt;% arrange(cstate)</code></pre>
<p>This section is copied straight from the MRP Primer. I’m not aware of a tidy way to handle this numerical coding of demographics. Let me know in the comments if you have some suggestions.</p>
<p>We’ll be joining the data based on the state number.</p>
<pre class="r"><code># add state level predictors to marriage.data
Statelevel &lt;- Statelevel %&gt;% rename(state.initnum = sstate.initnum)

marriage.data &lt;- Statelevel %&gt;%
  select(state.initnum, p.evang, p.mormon, kerry.04) %&gt;%
  right_join(marriage.data)

# Combine demographic groups
marriage.data &lt;- marriage.data %&gt;%
  mutate(race.female = (female * 3) + race.wbh) %&gt;%
  mutate(age.edu.cat = (female * 3) + race.wbh) %&gt;%
  mutate(p.relig = p.evang + p.mormon)

# add predictors
# Census$cstate.initnum &lt;-  match(Census$cstate, Statelevel$sstate)
# 
# Census$crace.female &lt;- (Census$cfemale * 3) + Census$crace.WBH
# Census$cage.edu.cat &lt;- 4 * (Census$cage.cat - 1) + Census$cedu.cat
# Census$cp.relig.full &lt;- Census$cp.evang.full + Census$cp.mormon.full
# 
# Census$cp.evang.full&lt;-  Statelevel$p.evang[Census$cstate.initnum]
# Census$cp.mormon.full &lt;- Statelevel$p.mormon[Census$cstate.initnum]
# 
# Census$cp.kerry.full &lt;-  Statelevel$kerry.04[Census$cstate.initnum]</code></pre>
</div>
</div>
<div id="model-1-disaggragation" class="section level2">
<h2>Model 1: Disaggragation</h2>
<pre class="r"><code>marriage.opinion &lt;- marriage.data %&gt;%
  group_by(statename) %&gt;%
  summarise(support = mean(yes.of.all))
marriage.opinion</code></pre>
<pre><code>## # A tibble: 50 x 2
##               statename   support
##                  &lt;fctr&gt;     &lt;dbl&gt;
##  1              alabama 0.1284404
##  2              arizona 0.4296875
##  3             arkansas 0.1071429
##  4           california 0.4624809
##  5             colorado 0.3923077
##  6          connecticut 0.3623188
##  7             delaware 0.1764706
##  8 district of columbia 0.2857143
##  9              florida 0.3227666
## 10              georgia 0.2692308
## # ... with 40 more rows</code></pre>
<p>As an aside, I really dislike this coding scheme as it seems to make the interpretation of the categorical quantities very difficult to understand. I’d like to try an alternative reformulation of the indicators later, or have a function to convert them a la <code>tidybayes</code> to easily understand.</p>
<p>Next, we code the system census in the same way.</p>
</div>
<div id="model-2-maximum-likelihood-multilevel-model" class="section level2">
<h2>Model 2: Maximum Likelihood Multilevel Model</h2>
<pre class="r"><code>approx.mod &lt;- glmer(formula = yes.of.all ~ (1|race.female) 
                    + (1|age.cat) + (1|edu.cat) + (1|age.edu.cat) 
                    + (1|state) + (1|region) + (1|poll) 
                    + p.relig + kerry.04,
                          data=marriage.data, family=binomial(link=&quot;logit&quot;))</code></pre>
<pre class="r"><code>summary(approx.mod)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: yes.of.all ~ (1 | race.female) + (1 | age.cat) + (1 | edu.cat) +  
##     (1 | age.edu.cat) + (1 | state) + (1 | region) + (1 | poll) +  
##     p.relig + kerry.04
##    Data: marriage.data
## 
##      AIC      BIC   logLik deviance df.resid 
##   7460.9   7528.4  -3720.4   7440.9     6331 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.2154 -0.7035 -0.4786  0.9909  3.8167 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  state       (Intercept) 0.001607 0.04009 
##  age.edu.cat (Intercept) 0.033869 0.18403 
##  race.female (Intercept) 0.020344 0.14263 
##  poll        (Intercept) 0.042019 0.20499 
##  region      (Intercept) 0.038809 0.19700 
##  edu.cat     (Intercept) 0.131696 0.36290 
##  age.cat     (Intercept) 0.305043 0.55231 
## Number of obs: 6341, groups:  
## state, 49; age.edu.cat, 6; race.female, 6; poll, 5; region, 5; edu.cat, 4; age.cat, 4
## 
## Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept) -1.397288   0.541549  -2.580  0.00988 **
## p.relig     -0.015162   0.005003  -3.031  0.00244 **
## kerry.04     0.018107   0.006943   2.608  0.00911 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##          (Intr) p.relg
## p.relig  -0.544       
## kerry.04 -0.718  0.653
## convergence code: 0
## Model failed to converge with max|grad| = 0.00105747 (tol = 0.001, component 1)
## Model is nearly unidentifiable: large eigenvalue ratio
##  - Rescale variables?</code></pre>
<p>First, note that I didn’t include the standard error from the MLE method. In general, this is <a href="https://stackoverflow.com/questions/31694812/standard-error-of-variance-component-from-the-output-of-lmer">hard to do</a>, but we get the percentile intervals for free with using <code>brms</code>.</p>
</div>
<div id="model-3-full-bayesian-model" class="section level2">
<h2>Model 3: Full Bayesian Model</h2>
<pre class="r"><code>bayes.mod &lt;- brm(yes.of.all ~ (1|race.female) + (1|age.cat) + (1|edu.cat)
                 + (1|age.edu.cat) + (1|state) + (1|region) + (1|poll)
                 + p.relig + kerry.04,
                 data=marriage.data, family=bernoulli(),
                 prior=c(set_prior(&quot;normal(0,0.2)&quot;, class=&#39;b&#39;),
                         set_prior(&quot;normal(0,0.2)&quot;, class=&#39;sd&#39;, group=&quot;race.female&quot;),
                         set_prior(&quot;normal(0,0.2)&quot;, class=&#39;sd&#39;, group=&quot;age.cat&quot;),
                         set_prior(&quot;normal(0,0.2)&quot;, class=&#39;sd&#39;, group=&quot;edu.cat&quot;),
                         set_prior(&quot;normal(0,0.2)&quot;, class=&#39;sd&#39;, group=&quot;age.edu.cat&quot;),
                         set_prior(&quot;normal(0,0.2)&quot;, class=&#39;sd&#39;, group=&quot;state&quot;),
                         set_prior(&quot;normal(0,0.2)&quot;, class=&#39;sd&#39;, group=&quot;region&quot;),
                         set_prior(&quot;normal(0,0.2)&quot;, class=&#39;sd&#39;, group=&quot;poll&quot;)))</code></pre>
<pre class="r"><code>summary(bayes.mod)</code></pre>
<pre><code>##  Family: bernoulli 
##   Links: mu = logit 
## Formula: yes.of.all ~ (1 | race.female) + (1 | age.cat) + (1 | edu.cat) + (1 | age.edu.cat) + (1 | state) + (1 | region) + (1 | poll) + p.relig + kerry.04 
##    Data: marriage.data (Number of observations: 6341) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; 
##          total post-warmup samples = 4000
##     ICs: LOO = NA; WAIC = NA; R2 = NA
##  
## Group-Level Effects: 
## ~age.cat (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.42      0.09     0.27     0.63       2784 1.00
## 
## ~age.edu.cat (Number of levels: 6) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.17      0.10     0.01     0.38       1430 1.00
## 
## ~edu.cat (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.32      0.09     0.18     0.52       3013 1.00
## 
## ~poll (Number of levels: 5) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.22      0.07     0.11     0.41       2443 1.00
## 
## ~race.female (Number of levels: 6) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.17      0.10     0.01     0.38       1438 1.00
## 
## ~region (Number of levels: 5) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.22      0.08     0.10     0.42       2878 1.00
## 
## ~state (Number of levels: 49) 
##               Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(Intercept)     0.07      0.04     0.00     0.16       1399 1.00
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept    -1.40      0.53    -2.46    -0.38       2263 1.00
## p.relig      -0.02      0.01    -0.03    -0.01       4000 1.00
## kerry.04      0.02      0.01     0.00     0.03       4000 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="model-comparisons" class="section level2">
<h2>Model Comparisons</h2>
<p>One comparison I like is what Matt Vuorre calls <a href="https://mvuorre.github.io/post/2017/within-subject-scatter/">with-subject scatterplots</a>. Although we are using them for a difference purpose here, the basic idea is really neat. Austin Rochford also uses a similar chart, but instead of dots I am going to plot the <a href="http://andrewgelman.com/2009/01/14/state-by-state/">two letter</a> <a href="http://andrewgelman.com/2014/05/16/gullibility-effect-using-state-level-correlations-draw-pretty-much-conclusion-want-individual-level-causation/">state abbreviation</a>.</p>
</div>
