---
title: MRP Using brms and tidybayes
author: ~
date: '2017-11-20'
slug: multilevel-mrp-tidybayes-brms-stan
categories: []
tags: []
description: Multilevel Regression and Poststratification with Stan
meta_img: /images/image.jpg
draft: true
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the-data">The Data</a><ul>
<li><a href="#cleaning-and-coding">Cleaning and coding</a></li>
</ul></li>
<li><a href="#model-1-disaggragation">Model 1: Disaggragation</a></li>
<li><a href="#model-2-maximum-likelihood-multilevel-model">Model 2: Maximum Likelihood Multilevel Model</a></li>
<li><a href="#model-3-full-bayesian-model">Model 3: Full Bayesian Model</a></li>
<li><a href="#model-comparisons">Model Comparisons</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In the last post I wrote the <a href="https://timmastny.netlify.com/blog/poststratification-with-dplyr/">“MRP Primer” Primer</a> studying the <em>p</em> part of MRP: poststratification. This post explores the actual <a href="http://www.princeton.edu/~jkastell/mrp_primer.html">MRP Primer by Jonathan Kastellec</a>. Jonathan and his coauthors wrote this excellent tutorial on Multilevel Regression and Poststratification (MRP) using <code>r-base</code> and <code>arm</code>/<code>lme4</code>.</p>
<p>Inspired by Austin Rochford’s full Bayesian implementation of the MRP Primer using <a href="https://gist.github.com/AustinRochford/bfc20cb3262169b41b730bd9faf74477">PyMC3</a>, I decided to approach the problem using R and <a href="http://mc-stan.org/">Stan</a>. In particular, I wanted to highlight two packages:</p>
<ul>
<li><p><a href="https://github.com/paul-buerkner/brms"><code>brms</code></a>, which provides a <code>lme4</code> like interface to Stan. And</p></li>
<li><p><a href="https://github.com/mjskay/tidybayes"><code>tidybayes</code></a>, which is a general tool for tidying Bayesian package outputs.</p></li>
</ul>
<p>Additionally, I’d like to compare the empirical mean disaggragated model, the maximum likelihood estimated multilevel model, the full Bayesian model.</p>
<p>I’d like to explore Jonathan’s example using some new tools. First, I’d like to explore the limitations of his approximate multilevel model with <code>glmer</code>, comparing it to a <code>Stan</code> model, which explores the whole posterior using the <code>brms</code> package.</p>
<p>Next, I want to compare the poststratified bayesian model to the disaggragated model.</p>
<p>Second, I’d like to tidy the workflow, especially using model tidying tools such as <code>broom</code> and <a href="https://github.com/mjskay/tidybayes"><code>tidybayes</code></a>.</p>
<p>Lastly, I want to work through some <code>R</code> mapping examples, using <code>albersusa</code> to visualize some of the results.</p>
<p>has a similar treatment using that I recommend you check out for an alternative perspective.</p>
<p>If our goal is to estimate state-level opinion, a simple way is to combine results from many different surveys and calculate the observed mean.</p>
<pre class="r"><code>library(tidyverse)
library(lme4)
library(brms)
library(rstan)
library(httr)

rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores())</code></pre>
</div>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<pre class="r"><code>marriage.data &lt;- foreign::read.dta(&#39;gay_marriage_megapoll.dta&#39;,
                                   convert.underscore=TRUE)
Statelevel &lt;- foreign::read.dta(&quot;state_level_update.dta&quot;,
                                convert.underscore = TRUE)
Census &lt;- foreign::read.dta(&quot;poststratification 2000.dta&quot;,
                            convert.underscore = TRUE)</code></pre>
<p>The first issue here is that survey respondants may not be in proportion to the actual population in the state. We can try to correct this with poststratification.</p>
<p>Second, the variance of undersampled groups is too large for an unbiased (observed) estimated mean to be useful. We can alleivate this by partial pooling used in multilevel regression.</p>
<p>Kastellec does a lot of variable recoding and in some ways I’m not sure the tidy way is much better, but we’ll try:</p>
<div id="cleaning-and-coding" class="section level3">
<h3>Cleaning and coding</h3>
<pre class="r"><code>Statelevel &lt;- Statelevel[order(Statelevel$sstate.initnum),]
Census &lt;- Census[order(Census$cstate),]
Census$cstate.initnum &lt;-  match(Census$cstate, Statelevel$sstate)</code></pre>
<pre class="r"><code>marriage.data$race.female &lt;- (marriage.data$female *3) + marriage.data$race.wbh
marriage.data$age.edu.cat &lt;- 4 * (marriage.data$age.cat -1) + marriage.data$edu.cat
marriage.data$p.evang.full &lt;- Statelevel$p.evang[marriage.data$state.initnum]
marriage.data$p.mormon.full &lt;-Statelevel$p.mormon[marriage.data$state.initnum]
marriage.data$p.relig.full &lt;- marriage.data$p.evang.full + marriage.data$p.mormon.full
marriage.data$p.kerry.full &lt;- Statelevel$kerry.04[marriage.data$state.initnum]

Census$crace.female &lt;- (Census$cfemale *3) + Census$crace.WBH
Census$cage.edu.cat &lt;- 4 * (Census$cage.cat -1) + Census$cedu.cat
Census$cp.evang.full&lt;-  Statelevel$p.evang[Census$cstate.initnum]
Census$cp.mormon.full &lt;- Statelevel$p.mormon[Census$cstate.initnum]
Census$cp.relig.full &lt;- Census$cp.evang.full + Census$cp.mormon.full
Census$cp.kerry.full &lt;-  Statelevel$kerry.04[Census$cstate.initnum]</code></pre>
</div>
</div>
<div id="model-1-disaggragation" class="section level2">
<h2>Model 1: Disaggragation</h2>
<pre class="r"><code>marriage.opinion &lt;- marriage.data %&gt;%
  group_by(statename) %&gt;%
  summarise(support = mean(yes.of.all))
marriage.opinion</code></pre>
<pre><code>## # A tibble: 50 x 2
##               statename   support
##                  &lt;fctr&gt;     &lt;dbl&gt;
##  1              alabama 0.1284404
##  2              arizona 0.4296875
##  3             arkansas 0.1071429
##  4           california 0.4624809
##  5             colorado 0.3923077
##  6          connecticut 0.3623188
##  7             delaware 0.1764706
##  8 district of columbia 0.2857143
##  9              florida 0.3227666
## 10              georgia 0.2692308
## # ... with 40 more rows</code></pre>
<p>As an aside, I really dislike this coding scheme as it seems to make the interpretation of the categorical quantities very difficult to understand. I’d like to try an alternative reformulation of the indicators later, or have a function to convert them a la <code>tidybayes</code> to easily understand.</p>
<p>Next, we code the system census in the same way.</p>
</div>
<div id="model-2-maximum-likelihood-multilevel-model" class="section level2">
<h2>Model 2: Maximum Likelihood Multilevel Model</h2>
<pre class="r"><code>approx.mod &lt;- glmer(formula = yes.of.all ~
                            (1|race.female) + (1|age.cat) +
                            (1|edu.cat) + (1|age.edu.cat) +
                            (1|state) + (1|region) + (1|poll) +
                            p.relig.full + p.kerry.full,
                          data=marriage.data, family=binomial(link=&quot;logit&quot;))</code></pre>
<pre class="r"><code>summary(approx.mod)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: yes.of.all ~ (1 | race.female) + (1 | age.cat) + (1 | edu.cat) +  
##     (1 | age.edu.cat) + (1 | state) + (1 | region) + (1 | poll) +  
##     p.relig.full + p.kerry.full
##    Data: marriage.data
## 
##      AIC      BIC   logLik deviance df.resid 
##   7459.4   7527.0  -3719.7   7439.4     6331 
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.166 -0.705 -0.477  0.981  3.884 
## 
## Random effects:
##  Groups      Name        Variance Std.Dev.
##  state       (Intercept) 0.001332 0.03649 
##  age.edu.cat (Intercept) 0.007712 0.08782 
##  race.female (Intercept) 0.054566 0.23359 
##  poll        (Intercept) 0.042177 0.20537 
##  region      (Intercept) 0.038087 0.19516 
##  edu.cat     (Intercept) 0.129591 0.35999 
##  age.cat     (Intercept) 0.306982 0.55406 
## Number of obs: 6341, groups:  
## state, 49; age.edu.cat, 16; race.female, 6; poll, 5; region, 5; edu.cat, 4; age.cat, 4
## 
## Fixed effects:
##               Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  -1.406573   0.541074  -2.600  0.00933 **
## p.relig.full -0.015250   0.004990  -3.056  0.00224 **
## p.kerry.full  0.018258   0.006922   2.638  0.00835 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) p.rlg.
## p.relig.fll -0.543       
## p.kerry.fll -0.716  0.653
## convergence code: 0
## Model failed to converge with max|grad| = 0.00102765 (tol = 0.001, component 1)</code></pre>
<p>First, note that I didn’t include the standard error from the MLE method. In general, this is <a href="https://stackoverflow.com/questions/31694812/standard-error-of-variance-component-from-the-output-of-lmer">hard to do</a>, but we get the percentile intervals for free with using <code>brms</code>.</p>
</div>
<div id="model-3-full-bayesian-model" class="section level2">
<h2>Model 3: Full Bayesian Model</h2>
<pre class="r"><code># bayes.mod &lt;- brm(yes.of.all ~ (1|race.female) + (1|age.cat) + (1|edu.cat) 
#                  + (1|age.edu.cat) + (1|state) + (1|region) + (1|poll) 
#                  + p.relig.full + p.kerry.full, 
#                  data=marriage.data, family=bernoulli(),
#                  prior=c(set_prior(&quot;normal(0,1)&quot;, class=&#39;b&#39;),
#                          set_prior(&quot;normal(0,1)&quot;, class=&#39;sd&#39;, group=&quot;race.female&quot;),
#                          set_prior(&quot;normal(0,1)&quot;, class=&#39;sd&#39;, group=&quot;age.cat&quot;),
#                          set_prior(&quot;normal(0,1)&quot;, class=&#39;sd&#39;, group=&quot;edu.cat&quot;),
#                          set_prior(&quot;normal(0,1)&quot;, class=&#39;sd&#39;, group=&quot;age.edu.cat&quot;),
#                          set_prior(&quot;normal(0,1)&quot;, class=&#39;sd&#39;, group=&quot;state&quot;),
#                          set_prior(&quot;normal(0,1)&quot;, class=&#39;sd&#39;, group=&quot;region&quot;),
#                          set_prior(&quot;normal(0,1)&quot;, class=&#39;sd&#39;, group=&quot;poll&quot;)))</code></pre>
</div>
<div id="model-comparisons" class="section level2">
<h2>Model Comparisons</h2>
<p>One comparison I like is what Matt Vuorre calls <a href="https://mvuorre.github.io/post/2017/within-subject-scatter/">with-subject scatterplots</a>. Although we are using them for a difference purpose here, the basic idea is really neat. Austin Rochford also uses a similar chart, but instead of dots I am going to plot the <a href="http://andrewgelman.com/2009/01/14/state-by-state/">two letter</a> <a href="http://andrewgelman.com/2014/05/16/gullibility-effect-using-state-level-correlations-draw-pretty-much-conclusion-want-individual-level-causation/">state abbreviation</a>.</p>
</div>
